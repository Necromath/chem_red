# -*- coding: utf-8 -*-
"""Another copy of Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rf4au64dBfm_3TXiAJMDO_QDTGBOjS4v
"""

"""
Factor Graph Belief Propagation Implementation

This module implements the Sum-Product Belief Propagation algorithm for factor graphs,
particularly focused on Ising models. The algorithm computes marginal probability
distributions (beliefs) for random variables connected by factors in a graphical model.
"""

import numpy as np
from collections import defaultdict
from itertools import product

# ========== UTILITY FUNCTIONS ==========

def normalize(vec):
    """
    Normalize a vector so its elements sum to 1 (probability distribution).

    Args:
        vec: numpy array to normalize

    Returns:
        Normalized array. If sum is 0, returns uniform distribution.
    """
    s = np.sum(vec)
    # If sum > 0, divide by sum; otherwise return uniform distribution
    return vec/s if s > 0 else np.full_like(vec, 1.0/len(vec))


def normalize_table(tbl):
    """
    Normalize a multi-dimensional table so all elements sum to 1.

    Args:
        tbl: numpy array of any shape

    Returns:
        Normalized array. If sum is 0, returns uniform distribution.
    """
    s = tbl.sum()
    # Normalize by total sum, or return uniform if sum is 0
    return tbl/s if s > 0 else np.full_like(tbl, 1.0/tbl.size)


def ising_unary(h, dom=('+','-')):
    """
    Create a unary Ising factor (external field): ψ(x) = exp(h * s)
    where s = +1 for '+' state and s = -1 for '-' state.

    Args:
        h: field strength parameter
        dom: domain tuple, typically ('+', '-')

    Returns:
        Dictionary mapping each state to its unnormalized probability weight
    """
    # Positive state gets exp(+h), negative state gets exp(-h)
    return {dom[0]: np.exp(+h), dom[1]: np.exp(-h)}


def ising_pair(J, dom=('+','-')):
    """
    Create a pairwise Ising factor: ψ(xi, xj) = exp(J * si * sj)
    where si, sj ∈ {+1, -1} are spin values.

    Args:
        J: coupling strength (positive = prefer same spin, negative = prefer opposite)
        dom: domain tuple, typically ('+', '-')

    Returns:
        Dictionary mapping each pair of states to its unnormalized weight
    """
    # Helper to convert symbol to spin value: '+' → +1, '-' → -1
    def sgn(x):
        return +1 if x == dom[0] else -1

    # Generate all combinations: ('+','+'), ('+','-'), ('-','+'), ('-','-')
    # Weight is exp(J * s_i * s_j)
    return {(a, b): np.exp(J * sgn(a) * sgn(b)) for a in dom for b in dom}


# ========== FACTOR GRAPH DATA STRUCTURES ==========

class Factor:
    """
    Represents a factor (potential function) in a factor graph.

    A factor connects one or more variables and assigns an unnormalized
    probability weight to each joint assignment of those variables.
    """

    def __init__(self, name, scope, psi):
        """
        Args:
            name: unique identifier for this factor
            scope: tuple of variable names this factor depends on, e.g., ('s1', 's2')
            psi: dictionary mapping assignments to positive weights
                 - For unary: {'+': w1, '-': w2}
                 - For pairwise: {('+','+'): w1, ('+','-'): w2, ...}
        """
        self.name = name
        self.scope = tuple(scope)  # Variables this factor connects
        self.psi = dict(psi)       # Assignment → weight mapping


class FactorGraph:
    """
    Container for a complete factor graph specification.

    A factor graph is a bipartite graph with variable nodes and factor nodes.
    It represents a factorized probability distribution:
    P(x) ∝ ∏_factors ψ_factor(x_scope)
    """

    def __init__(self, variables, domains, factors, unaries=None):
        """
        Args:
            variables: list of variable names (strings), e.g., ['s1', 's2', 's3']
            domains: dict mapping each variable to its possible values, e.g., {'s1': ('+','-')}
            factors: list of Factor objects (can be any arity ≥ 1)
            unaries: optional dict mapping variables to unary potentials (will be wrapped as factors)
        """
        self.vars = list(variables)
        # Ensure domains are tuples for consistent indexing
        self.dom = {v: tuple(domains[v]) for v in variables}
        self.factors = list(factors)

        # If unary potentials provided separately, wrap them as Factor objects
        if unaries:
            for v, psi in unaries.items():
                self.factors.append(Factor(f"unary_{v}", (v,), psi))

        # Build adjacency lists for the bipartite graph
        # Nv[variable] = set of factor names connected to this variable
        self.Nv = {v: set() for v in self.vars}
        # Na[factor_name] = set of variables in this factor's scope
        self.Na = {}

        for f in self.factors:
            self.Na[f.name] = set(f.scope)
            # For each variable in this factor, record the connection
            for v in f.scope:
                self.Nv[v].add(f.name)

        # Quick lookup: factor name → Factor object
        self.factor_by_name = {f.name: f for f in self.factors}


# ========== SUM-PRODUCT BELIEF PROPAGATION ALGORITHM ==========

def run_sum_product(fg: FactorGraph, max_iter=500, tol=1e-10, damping=0.5, verbose=False):
    """
    Execute the Sum-Product Belief Propagation algorithm on a factor graph.

    This algorithm computes approximate marginal probability distributions (beliefs)
    for each variable and factor through iterative message passing. Messages flow
    between variable nodes and factor nodes until convergence.

    Args:
        fg: FactorGraph object to run inference on
        max_iter: maximum number of message-passing iterations
        tol: convergence threshold (max change in any message)
        damping: damping factor ∈ [0,1] to stabilize convergence
                 new_message = (1-damping)*update + damping*old_message
        verbose: if True, print iteration details

    Returns:
        tuple of (var_beliefs, factor_beliefs, num_iterations):
        - var_beliefs: dict mapping variable → probability array over its domain
        - factor_beliefs: dict mapping factor_name → probability table over joint assignments
        - num_iterations: number of iterations until convergence
    """
    dom = fg.dom

    # ===== PREPROCESSING: Convert factors to efficient numpy arrays =====

    # Store the variable ordering for each factor
    f_order = {}
    # Store the factor potential as a numpy array
    f_table = {}

    for f in fg.factors:
        order = tuple(f.scope)  # e.g., ('s1', 's2')
        f_order[f.name] = order

        # Create array with shape corresponding to domain sizes
        # e.g., if s1 and s2 each have 2 values, shape is (2, 2)
        shape = tuple(len(dom[v]) for v in order)
        T = np.zeros(shape, dtype=float)

        # Build index maps: for each variable, map domain value to integer index
        # e.g., {'+': 0, '-': 1}
        idx_map = {v: {val: i for i, val in enumerate(dom[v])} for v in order}

        # Fill the array with factor weights
        for asg, w in f.psi.items():
            if len(order) == 1:
                # Unary factor: extract the single value
                v = order[0]
                if isinstance(asg, dict):
                    val = asg[v]
                elif isinstance(asg, tuple):
                    val = asg[0]
                else:
                    val = asg
                T[(idx_map[v][val],)] = float(w)
            else:
                # Pairwise (or higher-order) factor
                if isinstance(asg, dict):
                    # Convert dict assignment to tuple
                    key = tuple(asg[v] for v in order)
                else:
                    key = asg
                # Map symbolic assignment to array indices
                idx = tuple(idx_map[v][val] for v, val in zip(order, key))
                T[idx] = float(w)

        f_table[f.name] = T

    # ===== FOLD IN UNARY FACTORS =====
    # For efficiency, unary factors are folded directly into variable potentials
    # rather than being passed as messages

    # Initialize each variable with uniform potential
    var_phi = {v: np.ones(len(dom[v])) for v in fg.vars}

    # Multiply in any unary factors
    for f in fg.factors:
        if len(f.scope) == 1:
            v = f.scope[0]
            # Extract unary potential as array in domain order
            var_phi[v] *= np.array([f.psi[val] for val in dom[v]], dtype=float)

    # Support for dynamic unary overrides (e.g., from retractions/observations)
    # This allows external code to clamp or bias variables
    if hasattr(fg, "_phi_override"):
        for v, arr in fg._phi_override.items():
            if v in var_phi:
                var_phi[v] *= arr

    # ===== INITIALIZE MESSAGES =====
    # We only pass messages for non-unary factors (arity ≥ 2)
    # Messages are represented as numpy arrays (probability distributions)

    # m_f2v[(factor_name, var)] = message from factor to variable
    # m_v2f[(var, factor_name)] = message from variable to factor
    m_f2v, m_v2f = {}, {}

    for f in fg.factors:
        # Skip unary factors (already folded into var_phi)
        if len(f.scope) < 2:
            continue
        for v in f.scope:
            # Initialize all messages to uniform (all 1s)
            m_f2v[(f.name, v)] = np.ones(len(dom[v]))
            m_v2f[(v, f.name)] = np.ones(len(dom[v]))

    # ===== MESSAGE UPDATE FUNCTIONS =====

    def update_v2f(v, a):
        """
        Compute the message from variable v to factor a.

        Message = (unary potential) × ∏_{factors b ≠ a} (message from b to v)

        Args:
            v: variable name
            a: factor name

        Returns:
            Normalized message array
        """
        # Start with the unary potential (includes observations/fields)
        incoming = var_phi[v].copy()

        # Multiply by all incoming factor messages except from factor a
        for b in fg.Nv[v]:  # Iterate over factors connected to v
            # Skip factor a (don't use its message in the outgoing message to itself)
            # Skip unary factors (already in var_phi)
            if b == a or len(fg.factor_by_name[b].scope) < 2:
                continue
            incoming *= m_f2v[(b, v)]

        return normalize(incoming)

    def update_f2v(a, v):
        """
        Compute the message from factor a to variable v.

        Message[v=x] = ∑_{other variables} ψ_a(x, others) × ∏(messages from others to a)

        This is a sum over all joint assignments to the factor's variables except v,
        weighted by the factor potential and incoming messages.

        Args:
            a: factor name
            v: variable name

        Returns:
            Normalized message array
        """
        order = f_order[a]     # Variable ordering in the factor
        ftab = f_table[a]      # Factor potential as numpy array

        # Map variables to their axis indices in the array
        axes = {var: i for i, var in enumerate(order)}
        v_axis = axes[v]       # The axis we'll marginalize to (keep this one)

        # Multiply factor table by all incoming messages (except from v)
        msg_prod = 1.0
        for u in order:
            if u == v:
                continue  # Don't include v's message (this is the belief propagation rule)

            m = m_v2f[(u, a)]  # Get incoming message from variable u

            # Reshape message to broadcast along the correct axis
            # e.g., if u is axis 1 in a 2D array, reshape (2,) → (1, 2)
            shape = [1] * ftab.ndim
            shape[axes[u]] = len(m)
            msg_prod = msg_prod * m.reshape(shape)

        # Sum out all variables except v (marginalization)
        # This computes the message: sum over all axes except v_axis
        num = (ftab * msg_prod).sum(axis=tuple(i for i in range(ftab.ndim) if i != v_axis))

        return normalize(num)

    # ===== ITERATIVE MESSAGE PASSING =====

    for it in range(max_iter):
        max_delta = 0.0  # Track maximum change for convergence check

        # --- UPDATE ALL VARIABLE-TO-FACTOR MESSAGES ---
        new_v2f = {}
        for v in fg.vars:
            for a in fg.Nv[v]:  # For each factor connected to v
                # Skip unary factors
                if len(fg.factor_by_name[a].scope) < 2:
                    continue

                # Compute new message
                m_new = update_v2f(v, a)
                key = (v, a)
                m_old = m_v2f[key]

                # Apply damping: interpolate between new and old message
                # This helps stabilize convergence, especially in loopy graphs
                m_dmp = normalize((1 - damping) * m_new + damping * m_old)
                new_v2f[key] = m_dmp

                # Track maximum change (for convergence criterion)
                max_delta = max(max_delta, float(np.max(np.abs(m_dmp - m_old))))

        # Update all variable-to-factor messages simultaneously (synchronous update)
        m_v2f.update(new_v2f)

        # --- UPDATE ALL FACTOR-TO-VARIABLE MESSAGES ---
        new_f2v = {}
        for f in fg.factors:
            # Skip unary factors
            if len(f.scope) < 2:
                continue

            for v in f.scope:
                # Compute new message
                m_new = update_f2v(f.name, v)
                key = (f.name, v)
                m_old = m_f2v[key]

                # Apply damping
                m_dmp = normalize((1 - damping) * m_new + damping * m_old)
                new_f2v[key] = m_dmp

                # Track maximum change
                max_delta = max(max_delta, float(np.max(np.abs(m_dmp - m_old))))

        # Update all factor-to-variable messages simultaneously
        m_f2v.update(new_f2v)

        # Check for convergence: if all messages changed by less than tolerance, stop
        if max_delta < tol:
            break

    # ===== COMPUTE FINAL BELIEFS =====

    # Variable beliefs: multiply unary potential by all incoming factor messages
    var_beliefs = {}
    for v in fg.vars:
        b = var_phi[v].copy()  # Start with unary potential

        # Multiply by all incoming factor messages
        for a in fg.Nv[v]:
            if len(fg.factor_by_name[a].scope) >= 2:  # Only non-unary factors
                b *= m_f2v[(a, v)]

        var_beliefs[v] = normalize(b)

    # Factor beliefs: multiply factor potential by all incoming variable messages
    factor_beliefs = {}
    for f in fg.factors:
        # Special case: unary factors
        if len(f.scope) == 1:
            v = f.scope[0]
            # For unary, belief is just the variable's unary potential
            factor_beliefs[f.name] = normalize(var_phi[v])
            continue

        # General case: multiply factor table by incoming messages
        order = f_order[f.name]
        ftab = f_table[f.name]

        msg_prod = 1.0
        for i, var in enumerate(order):
            m = m_v2f[(var, f.name)]  # Get message from each variable

            # Reshape to broadcast correctly
            shape = [1] * ftab.ndim
            shape[i] = len(m)
            msg_prod = msg_prod * m.reshape(shape)

        # Factor belief = factor potential × incoming messages
        B = ftab * msg_prod
        factor_beliefs[f.name] = normalize_table(B)

    return var_beliefs, factor_beliefs, it + 1


# ========== EXAMPLE FACTOR GRAPHS ==========
# These examples demonstrate different graph topologies with Ising models

def build_example_A():
    """Triangle graph (3-cycle) with one leaf node and a unary factor."""
    dom = {v: ('+', '-') for v in ['s1', 's2', 's3', 's4']}

    # Triangle: s1-s2-s3-s1
    f12 = Factor('f12', ('s1', 's2'), ising_pair(J=0.4))
    f23 = Factor('f23', ('s2', 's3'), ising_pair(J=0.6))
    f31 = Factor('f31', ('s3', 's1'), ising_pair(J=0.5))

    # Leaf edge: s3-s4
    f34 = Factor('f34', ('s3', 's4'), ising_pair(J=0.7))

    # External field on s1
    un = {'s1': ising_unary(h=0.3)}

    fg = FactorGraph(
        variables=['s1', 's2', 's3', 's4'],
        domains=dom,
        factors=[f12, f23, f31, f34],
        unaries=un
    )
    return fg


def build_example_B():
    """4-cycle (square) with one leaf node and a unary factor."""
    dom = {v: ('+', '-') for v in ['s1', 's2', 's3', 's4', 's5']}

    # Square cycle: s1-s2-s3-s4-s1
    f12 = Factor('f12', ('s1', 's2'), ising_pair(J=0.4))
    f23 = Factor('f23', ('s2', 's3'), ising_pair(J=0.5))
    f34 = Factor('f34', ('s3', 's4'), ising_pair(J=0.45))
    f41 = Factor('f41', ('s4', 's1'), ising_pair(J=0.55))

    # Leaf on s1: adds s5
    f15 = Factor('f15', ('s1', 's5'), ising_pair(J=0.6))

    # Negative external field on s3 (favors '-' state)
    un = {'s3': ising_unary(h=-0.2)}

    fg = FactorGraph(
        variables=['s1', 's2', 's3', 's4', 's5'],
        domains=dom,
        factors=[f12, f23, f34, f41, f15],
        unaries=un
    )
    return fg


def build_example_C():
    """Simple 3-node chain with a unary factor."""
    dom = {v: ('+', '-') for v in ['s1', 's2', 's3']}

    # Chain: s1-s2-s3
    f12 = Factor('f12', ('s1', 's2'), ising_pair(J=0.4))
    f23 = Factor('f23', ('s2', 's3'), ising_pair(J=0.5))

    # Negative field on s1
    un = {'s1': ising_unary(h=-0.2)}

    fg = FactorGraph(
        variables=['s1', 's2', 's3'],
        domains=dom,
        factors=[f12, f23],
        unaries=un
    )
    return fg


def build_example_D():
    """4-cycle with strong positive unary fields on all nodes."""
    dom = {v: ('+', '-') for v in ['s1', 's2', 's3', 's4']}

    # Cycle with mixed coupling strengths (one negative, others positive)
    f12 = Factor('f12', ('s1', 's2'), ising_pair(J=0.4))
    f23 = Factor('f23', ('s2', 's3'), ising_pair(J=-0.3))  # Antiferromagnetic
    f34 = Factor('f34', ('s3', 's4'), ising_pair(J=0.2))
    f41 = Factor('f41', ('s4', 's1'), ising_pair(J=0.1))

    # Strong positive fields on all variables (favor '+' state)
    unaries = {
        's1': ising_unary(h=1.0),
        's2': ising_unary(h=1.0),
        's3': ising_unary(h=1.0),
        's4': ising_unary(h=1.0),
    }

    fg = FactorGraph(
        variables=['s1', 's2', 's3', 's4'],
        domains=dom,
        factors=[f12, f23, f34, f41],
        unaries=unaries
    )
    return fg


def build_example_E():
    """3-node chain with strong positive unary fields."""
    dom = {v: ('+', '-') for v in ['s1', 's2', 's3']}

    # Chain with one antiferromagnetic coupling
    f12 = Factor('f12', ('s1', 's2'), ising_pair(J=0.4))
    f23 = Factor('f23', ('s2', 's3'), ising_pair(J=-0.3))

    # Positive fields on all variables
    unaries = {
        's1': ising_unary(h=1),
        's2': ising_unary(h=1),
        's3': ising_unary(h=1),
    }

    fg = FactorGraph(
        variables=['s1', 's2', 's3'],
        domains=dom,
        factors=[f12, f23],
        unaries=unaries
    )
    return fg


def build_example_F():
    """3-node chain with no unary factors (pure pairwise interactions)."""
    dom = {v: ('+', '-') for v in ['s1', 's2', 's3']}

    # Chain with antiferromagnetic then ferromagnetic coupling
    f12 = Factor('f12', ('s1', 's2'), ising_pair(J=-0.5))  # Prefer opposite spins
    f23 = Factor('f23', ('s2', 's3'), ising_pair(J=0.7))   # Prefer same spins

    fg = FactorGraph(
        variables=['s1', 's2', 's3'],
        domains=dom,
        factors=[f12, f23]
    )
    return fg


# ========== MAIN EXECUTION ==========

if __name__ == "__main__":
    # Run belief propagation on all example graphs
    examples = [
        ("Example A (triangle+leaf+unary)", build_example_A),
        ("Example B (square+leaf+unary)", build_example_B),
        ("Example C (chain+unary)", build_example_C),
        ("Example D (cycle+ full unary)", build_example_D),
        ("Example E (chain+ full unary)", build_example_E),
        ("Example F (chain+ no unary)", build_example_F)
    ]

    for name, builder in examples:
        print(f"\n=== {name} ===")

        # Build the factor graph
        fg = builder()

        # Run sum-product belief propagation
        vb, fb, iters = run_sum_product(
            fg,
            max_iter=1000,
            tol=1e-12,      # Very tight convergence
            damping=0.3     # Light damping for stability
        )

        print(f"Converged in {iters} iterations")

        # Print variable beliefs (marginal probabilities)
        for v in fg.vars:
            plus_idx = 0  # First index corresponds to '+' state
            print(f"  b({v})[+]={vb[v][plus_idx]:.6f}, [-]={vb[v][1-plus_idx]:.6f}")

        # Print factor beliefs (joint probabilities for small factors)
        for f in fg.factors:
            order = f.scope
            B = fb[f.name]

            if len(order) == 1:
                # Unary factor: show both states
                vals = fg.dom[order[0]]
                print(f"  b({f.name})[{order[0]}=+]={B[0]:.6f}, [{order[0]}=-]={B[1]:.6f}")
            else:
                # Pairwise factor: show all four joint states
                vals0 = fg.dom[order[0]]
                vals1 = fg.dom[order[1]]
                print(f"  b({f.name})[{order[0]}=+, {order[1]}=+]={B[0,0]:.6f}, "
                      f"[+, -]={B[0,1]:.6f}, [-, +]={B[1,0]:.6f}, [-, -]={B[1,1]:.6f}")

"""
Chemical Reaction Network (CRN) Implementation of Belief Propagation

This module implements the Napp-Adams approach for performing belief propagation
using chemical reaction networks. Instead of passing messages as arrays, messages
are represented as chemical species concentrations that evolve via ODEs to compute
the same marginal probabilities as standard BP.

The key insight: message passing can be realized through catalytic chemical reactions,
where species concentrations naturally converge to belief propagation fixed points.
"""

import numpy as np
from itertools import product
from scipy.integrate import solve_ivp


# ========== PREPROCESSING UTILITIES ==========

def pack_factor_tables(fg):
    """
    Convert factor graph potentials into efficient numpy arrays for CRN computation.

    This is the same preprocessing used in the BP code, ensuring consistency between
    the two implementations.

    Args:
        fg: FactorGraph object

    Returns:
        tuple of (order, table, idx_of):
        - order: dict mapping factor_name → tuple of variable names in scope
        - table: dict mapping factor_name → numpy array of potential values
        - idx_of: dict mapping factor_name → {var → {value → index}}
    """
    order = {}
    table = {}
    idx_of = {}

    for f in fg.factors:
        sc = tuple(f.scope)  # e.g., ('s1', 's2')
        order[f.name] = sc

        # Build index mapping: for each variable, map domain values to array indices
        # e.g., {'+': 0, '-': 1}
        idx_of[f.name] = {v: {val: i for i, val in enumerate(fg.dom[v])} for v in sc}

        # Create array with appropriate shape
        shape = tuple(len(fg.dom[v]) for v in sc)
        T = np.zeros(shape, dtype=float)

        # Fill array with factor potential values
        for asg, w in f.psi.items():
            if len(sc) == 1:
                # Unary factor: handle different assignment formats
                v = sc[0]
                if isinstance(asg, dict):
                    val = asg[v]
                elif isinstance(asg, tuple):
                    val = asg[0]
                else:
                    val = asg
                T[(idx_of[f.name][v][val],)] = float(w)
            else:
                # Pairwise (or higher-order) factor
                if isinstance(asg, dict):
                    # Convert dict to tuple in scope order
                    vals = tuple(asg[v] for v in sc)
                else:
                    # Assume flat tuple already in scope order
                    vals = asg
                # Map symbolic values to array indices
                idx = tuple(idx_of[f.name][v][val] for v, val in zip(sc, vals))
                T[idx] = float(w)

        table[f.name] = T

    return order, table, idx_of


# ========== NAPP-ADAMS CRN FOR BELIEF PROPAGATION ==========

class NappCRN:
    r"""
    Chemical Reaction Network implementing Belief Propagation via Napp-Adams construction.

    For binary-domain factor graphs (each variable has 2 states, typically '+' and '-'),
    this CRN uses chemical species to represent BP messages and marginals.

    SPECIES DESIGN (all use "triplet" representation):
    Each message/marginal is encoded as 3 species: {0, +, -}
    - '0' species: "zero pool" - unconverted substrate
    - '+' species: mass representing belief/message value for positive state
    - '-' species: mass representing belief/message value for negative state

    The ratio +/(+ + -) gives the normalized probability for the '+' state.

    SPECIES TYPES:
    1. S_{i->i}: Unary potential for variable i (triplet: 0, +, -)
       Represents the local evidence/field at variable i

    2. S_{a->i}: Sum message from factor a to variable i (triplet: 0, +, -)
       Corresponds to factor-to-variable messages in standard BP
       Only exists for non-unary factors (arity ≥ 2)

    3. P_{i->a}: Product message from variable i to factor a (triplet: 0, +, -)
       Corresponds to variable-to-factor messages in standard BP
       Only exists for non-unary factors

    4. M_i: Marginal belief for variable i (triplet: 0, +, -)
       Final output representing P(x_i)

    REACTIONS:
    1. Recycling: converts + and - species back to 0 (maintains conservation)
    2. Sum updates: converts 0 → {+,-} based on factor potentials and incoming messages
    3. Product updates: catalytic reactions computing message products
    4. Marginal updates: catalytic reactions computing final beliefs
    """

    def __init__(self, fg, kappa_r=0.5, kappa_prod=50.0):
        """
        Initialize the CRN for a given factor graph.

        Args:
            fg: FactorGraph object (must have binary domains)
            kappa_r: rate constant for recycling reactions (prevents depletion)
            kappa_prod: rate constant for product/marginal computation reactions
        """
        self.fg = fg
        self.kappa_r = kappa_r        # Recycling rate (0 pools replenishment)
        self.kappa_prod = kappa_prod  # Product computation rate

        # Preprocess factor potentials into numpy arrays
        self.f_order, self.f_table, self.f_idx = pack_factor_tables(fg)

        # Copy adjacency information from factor graph
        self.Nv = fg.Nv  # Nv[var] = set of factors connected to var
        self.Na = fg.Na  # Na[factor] = set of vars in factor's scope

        # ===== IDENTIFY UNARY FACTORS =====
        # Detect which variables have explicit unary factors
        # These will be used for S_{i->i} initialization
        self.unary_factor_of = {v: None for v in fg.vars}
        for f in fg.factors:
            if len(f.scope) == 1:
                self.unary_factor_of[f.scope[0]] = f.name

        # ===== ALLOCATE SPECIES =====
        # Each "triplet" gets 3 consecutive indices: 0, +, -
        idx = 0  # Current species index

        self.S_ii = {}  # S_{i->i}: unary potentials
        self.S_ai = {}  # S_{a->i}: factor-to-variable messages
        self.P_ia = {}  # P_{i->a}: variable-to-factor messages
        self.M_i = {}   # M_i: variable marginals

        def triplet():
            """Allocate 3 consecutive species indices and return their mapping."""
            nonlocal idx
            d = {'0': idx, '+': idx + 1, '-': idx + 2}
            idx += 3
            return d

        # Cache factor arities for quick lookup
        self.arity = {a: len(self.f_order[a]) for a in self.fg.factor_by_name}

        # --- Allocate S_{i->i} for all variables ---
        # These store the unary potentials (fields/observations)
        for i in fg.vars:
            self.S_ii[i] = triplet()

        # --- Allocate S_{a->i} ONLY for non-unary factors ---
        # Unary factors are folded into S_{i->i}, so they don't need S_{a->i}
        for a in self.fg.factor_by_name:
            if self.arity[a] >= 2:  # Only pairwise or higher-order factors
                for i in self.Na[a]:  # For each variable in factor's scope
                    self.S_ai[(a, i)] = triplet()

        # --- Allocate P_{i->a} ONLY toward non-unary factors ---
        # Variables don't send product messages to their own unary factors
        for i in fg.vars:
            for a in self.Nv[i]:  # For each factor connected to variable i
                if self.arity[a] >= 2:
                    self.P_ia[(i, a)] = triplet()

        # --- Allocate M_i for all variables ---
        # These accumulate the final marginal beliefs
        for i in fg.vars:
            self.M_i[i] = triplet()

        self.n_species = idx  # Total number of species

        # ===== PRECOMPUTE UNARY POTENTIALS =====
        # Extract unary ψ_i values for each variable (for S_{i->i} reactions)
        self.psi_ii = {}
        for i in fg.vars:
            a = self.unary_factor_of[i]
            if a is None:
                # No explicit unary factor → uniform (all 1s)
                self.psi_ii[i] = np.ones(2, dtype=float)
            else:
                # Extract unary factor potential as length-2 array
                self.psi_ii[i] = self.f_table[a].copy().reshape(2)


    def _all_zero_pool_indices(self):
        """
        Get indices of all '0' pool species for monitoring convergence.

        When all 0 pools are depleted (near zero), the system has reached
        equilibrium and all beliefs are computed.

        Returns:
            List of species indices corresponding to '0' pools
        """
        zeros = []
        # Collect '0' indices from all triplets
        for d in (list(self.S_ii.values()) +
                  list(self.S_ai.values()) +
                  list(self.P_ia.values()) +
                  list(self.M_i.values())):
            zeros.append(d['0'])
        return zeros


    def _recycling(self, dydt, y):
        """
        Implement recycling reactions for all species triplets.

        Recycling prevents complete depletion of the 0 pool, allowing the system
        to continue adjusting beliefs. The reactions are:

        '+' → '0'  at rate kappa_r
        '-' → '0'  at rate kappa_r

        This creates a balance: 0 pools are consumed by sum reactions and
        replenished by recycling, leading to equilibrium concentrations that
        encode the BP beliefs.

        Args:
            dydt: derivative array to update (in-place)
            y: current species concentrations
        """
        # Apply recycling to ALL triplets (S_ii, S_ai, P_ia, M_i)
        for d in (list(self.S_ii.values()) +
                  list(self.S_ai.values()) +
                  list(self.P_ia.values()) +
                  list(self.M_i.values())):
            # 0 pool increases as + and - flow back
            dydt[d['0']] += self.kappa_r * (y[d['+']] + y[d['-']])
            # + and - pools decrease at rate kappa_r
            dydt[d['+']] -= self.kappa_r * y[d['+']]
            dydt[d['-']] -= self.kappa_r * y[d['-']]


    def _sum_updates(self, dydt, y):
        r"""
        Implement sum update reactions (factor-to-variable message computation).

        These reactions convert 0 pool species into + or - based on:

        (A) UNARY: S_{i->i}(x_i) ← ψ_i(x_i)
            Reactions: 0 → + at rate ψ_i(+) * [0]
                       0 → - at rate ψ_i(-) * [0]

        (B) NON-UNARY: S_{a->i}(x_i) ← Σ_{x_{N(a)\i}} ψ_a(x) × Π_{u∈N(a)\i} P_{u→a}(x_u)
            This marginalizes the factor potential over all variables except i,
            weighted by incoming product messages from other variables.

            For each assignment of i (+ or -), we sum over all assignments to
            other variables in the factor, multiplying by:
            - Factor potential ψ_a(x)
            - Product messages from all other variables

        Args:
            dydt: derivative array to update (in-place)
            y: current species concentrations
        """

        # (A) UNARY FACTORS: Drive S_{i->i} pools
        for i in self.fg.vars:
            d = self.S_ii[i]
            psi = self.psi_ii[i]  # [ψ_i(+), ψ_i(-)] - length-2 array

            tot = psi[0] + psi[1]  # Total reaction rate out of 0 pool

            # 0 → + at rate ψ_i(+)
            # 0 → - at rate ψ_i(-)
            dydt[d['0']] -= tot * y[d['0']]
            dydt[d['+']] += psi[0] * y[d['0']]
            dydt[d['-']] += psi[1] * y[d['0']]

        # (B) NON-UNARY FACTORS: Drive S_{a->i} pools
        for a in self.fg.factor_by_name:
            if self.arity[a] < 2:
                continue  # Skip unary factors (handled above)

            scope = self.f_order[a]  # Ordered list of variables in factor
            T = self.f_table[a]      # Factor potential as numpy array

            # Map each variable to its position in the array
            pos_in = {v: k for k, v in enumerate(scope)}

            # For each variable i in the factor's scope
            for i in scope:
                d = self.S_ai[(a, i)]  # Species triplet for message a→i
                ax = pos_in[i]         # Array axis for variable i

                # Accumulate reaction rates for i=+ (index 0) and i=- (index 1)
                r = np.zeros(2, dtype=float)

                # Iterate over all joint assignments to variables in scope
                # Each tuple element is 0 (for +) or 1 (for -)
                for tup in product(*[range(2) for _ in scope]):
                    w = T[tuple(tup)]  # Factor potential for this assignment

                    if w == 0.0:
                        continue  # Skip zero potentials (no reaction)

                    # Compute product of incoming P messages from other variables
                    prod_msgs = 1.0
                    for u, iu in zip(scope, tup):
                        if u == i:
                            continue  # Don't include i's own message
                        # Multiply by P_{u→a} for the appropriate state
                        prod_msgs *= y[self.P_ia[(u, a)]['+' if iu == 0 else '-']]

                    # Accumulate rate for this assignment of i
                    r[tup[ax]] += w * prod_msgs

                tot = r.sum()  # Total outflow from 0 pool

                # Update rates: 0 → + and 0 → -
                dydt[d['0']] -= tot * y[d['0']]
                dydt[d['+']] += r[0] * y[d['0']]
                dydt[d['-']] += r[1] * y[d['0']]


    def _product_updates(self, dydt, y):
        r"""
        Implement product update reactions (variable-to-factor messages).

        These are CATALYTIC reactions that compute:
        P_{i→a}(x_i) ← S_{i→i}(x_i) × Π_{b∈N(i)\a, arity(b)≥2} S_{b→i}(x_i)

        The product message from variable i to factor a is the product of:
        - Unary potential S_{i→i}
        - All incoming S messages from other non-unary factors

        Catalytic mechanism:
        For state x_i ∈ {+, -}:
          0 + S_{i→i}(x_i) + S_{b₁→i}(x_i) + ... → + + S_{i→i}(x_i) + S_{b₁→i}(x_i) + ...

        Rate = kappa_prod × [0] × [S_{i→i}(x_i)] × Π_b [S_{b→i}(x_i)]

        The S species act as catalysts (not consumed), encoding the message product.

        Args:
            dydt: derivative array to update (in-place)
            y: current species concentrations
        """
        for i in self.fg.vars:
            for a in self.Nv[i]:  # For each factor connected to i
                if (i, a) not in self.P_ia:  # Only toward non-unary factors
                    continue

                dP = self.P_ia[(i, a)]  # Product message triplet

                # Compute reactions for both states: xi=0 ('+') and xi=1 ('-')
                for xi in (0, 1):
                    # Start with S_{i→i} as catalyst
                    catal = y[self.S_ii[i]['+' if xi == 0 else '-']]

                    # Multiply by all incoming S_{b→i} messages (except from a)
                    for b in self.Nv[i]:
                        if b == a or self.arity[b] < 2:
                            continue  # Skip factor a and unary factors
                        catal *= y[self.S_ai[(b, i)]['+' if xi == 0 else '-']]

                    # Catalytic reaction rate: kappa_prod × [0] × [catalysts]
                    rate = self.kappa_prod * y[dP['0']] * catal

                    # Convert 0 → appropriate state
                    if xi == 0:
                        dydt[dP['0']] -= rate
                        dydt[dP['+']] += rate
                    else:
                        dydt[dP['0']] -= rate
                        dydt[dP['-']] += rate


    def _marginal_updates(self, dydt, y):
        r"""
        Implement marginal computation reactions (final beliefs).

        These catalytic reactions compute the variable marginals:
        M_i(x_i) ← S_{i→i}(x_i) × Π_{b∈N(i), arity(b)≥2} S_{b→i}(x_i)

        Similar to product messages, but includes ALL incoming factor messages
        (no exclusion), computing the final belief b(x_i).

        Catalytic mechanism:
        For state x_i ∈ {+, -}:
          0 + S_{i→i}(x_i) + S_{b₁→i}(x_i) + ... → + + S_{i→i}(x_i) + S_{b₁→i}(x_i) + ...

        Rate = kappa_prod × [0] × [S_{i→i}(x_i)] × Π_b [S_{b→i}(x_i)]

        The final ratio M_i[+] / (M_i[+] + M_i[-]) gives P(x_i = +).

        Args:
            dydt: derivative array to update (in-place)
            y: current species concentrations
        """
        for i in self.fg.vars:
            dM = self.M_i[i]  # Marginal triplet for variable i

            # Compute reactions for both states: xi=0 ('+') and xi=1 ('-')
            for xi in (0, 1):
                # Start with S_{i→i} as catalyst
                catal = y[self.S_ii[i]['+' if xi == 0 else '-']]

                # Multiply by ALL incoming S_{b→i} messages from non-unary factors
                for b in self.Nv[i]:
                    if self.arity[b] < 2:
                        continue  # Skip unary factors (already in S_{i→i})
                    catal *= y[self.S_ai[(b, i)]['+' if xi == 0 else '-']]

                # Catalytic reaction rate
                rate = self.kappa_prod * y[dM['0']] * catal

                # Convert 0 → appropriate state
                if xi == 0:
                    dydt[dM['0']] -= rate
                    dydt[dM['+']] += rate
                else:
                    dydt[dM['0']] -= rate
                    dydt[dM['-']] += rate


    def ode(self, t, y):
        """
        Complete ODE system combining all reaction types.

        This function defines dy/dt for the entire CRN, combining:
        1. Recycling reactions (maintain mass)
        2. Sum reactions (compute factor messages)
        3. Product reactions (compute variable messages)
        4. Marginal reactions (compute beliefs)

        Args:
            t: current time (not used - autonomous system)
            y: current species concentrations (length n_species)

        Returns:
            dydt: time derivatives for all species
        """
        dydt = np.zeros(self.n_species, dtype=float)

        # Apply all reaction types
        self._recycling(dydt, y)
        self._sum_updates(dydt, y)
        self._product_updates(dydt, y)
        self._marginal_updates(dydt, y)

        return dydt


    def simulate(self, t_max=150.0, rtol=1e-9, atol=1e-12, eps=3e-6,
                 stop_when_zero_below=None):
        """
        Simulate the CRN dynamics until convergence or timeout.

        The simulation starts with all mass in 0 pools, with small perturbations
        to break symmetry. Over time, mass flows into + and - pools according to
        the BP message passing dynamics.

        Args:
            t_max: maximum simulation time
            rtol, atol: relative and absolute tolerances for ODE solver
            eps: small perturbation to break initial symmetry
            stop_when_zero_below: if set, stop early when max(0 pools) < threshold
                                  (indicates convergence - all mass has been distributed)

        Returns:
            solve_ivp result object containing:
            - t: time points
            - y: species concentrations over time
            - status: solver status (0 = success)
        """
        # ===== INITIALIZE CONCENTRATIONS =====
        y0 = np.zeros(self.n_species, dtype=float)

        # Start with all mass in 0 pools
        for d in (list(self.S_ii.values()) +
                  list(self.S_ai.values()) +
                  list(self.P_ia.values()) +
                  list(self.M_i.values())):
            y0[d['0']] = 1.0

        # Add small perturbations to + and - pools to break symmetry
        # This helps the system converge to the correct solution
        for d in (list(self.S_ii.values()) +
                  list(self.S_ai.values()) +
                  list(self.P_ia.values())):
            y0[d['+']] += eps
            y0[d['-']] += eps
            y0[d['0']] = max(1.0 - 2 * eps, 0.0)  # Adjust 0 pool accordingly

        # Smaller perturbation for marginals
        for d in self.M_i.values():
            y0[d['+']] += 0.5 * eps
            y0[d['-']] += 0.5 * eps
            y0[d['0']] = max(1.0 - eps, 0.0)

        # ===== SET UP EARLY STOPPING (OPTIONAL) =====
        if stop_when_zero_below is None:
            # No early stopping - just integrate to t_max
            return solve_ivp(self.ode, [0, t_max], y0,
                           method='LSODA', rtol=rtol, atol=atol)

        # Early stopping based on depletion of 0 pools
        zero_ids = self._all_zero_pool_indices()
        thresh = float(stop_when_zero_below)

        # Define event: max(0-pools) crosses threshold from above
        def ev_zero_drop(t, y):
            """Event function: returns 0 when max 0-pool hits threshold."""
            return np.max(y[zero_ids]) - thresh

        ev_zero_drop.terminal = True      # Stop integration when event occurs
        ev_zero_drop.direction = -1       # Only trigger on downward crossing

        # Check if already below threshold (shouldn't happen with proper initialization)
        if np.max(y0[zero_ids]) <= thresh:
            # Just run a tiny step to return a valid solution
            return solve_ivp(self.ode, [0, min(1e-6, t_max)], y0,
                           method='LSODA', rtol=rtol, atol=atol)

        # Run with event detection for early stopping
        return solve_ivp(self.ode, [0, t_max], y0,
                       method='LSODA', rtol=rtol, atol=atol,
                       events=ev_zero_drop)


    def read_marginals(self, y):
        """
        Extract normalized marginal beliefs from CRN concentrations.

        For each variable, compute P(+) = M_i[+] / (M_i[+] + M_i[-])
        and P(-) = 1 - P(+).

        Args:
            y: species concentration vector (typically final state from simulation)

        Returns:
            dict mapping variable_name → {state: probability}
            e.g., {'s1': {'+': 0.73, '-': 0.27}, ...}
        """
        out = {}
        for i in self.fg.vars:
            d = self.M_i[i]
            pos = y[d['+']]  # Concentration of + species
            neg = y[d['-']]  # Concentration of - species
            z = pos + neg    # Total mass (should be close to 1.0 at equilibrium)

            if z <= 0:
                # Degenerate case - return uniform
                p_plus = 0.5
            else:
                # Normalize to get probability
                p_plus = pos / z

            # Build result dict using actual domain symbols
            out[i] = {
                self.fg.dom[i][0]: p_plus,        # '+' state
                self.fg.dom[i][1]: 1.0 - p_plus   # '-' state
            }
        return out


# ========== VERIFICATION AND COMPARISON UTILITIES ==========

def read_normalized_triplet(y, d):
    """
    Extract normalized distribution from a species triplet.

    Given species concentrations y and triplet dict d = {'0': idx0, '+': idx1, '-': idx2},
    compute the normalized distribution [P(+), P(-)].

    Args:
        y: species concentration vector
        d: triplet dict mapping {'0', '+', '-'} to species indices

    Returns:
        numpy array [P(+), P(-)] normalized to sum to 1
    """
    tot = y[d['0']] + y[d['+']] + y[d['-']]
    if tot <= 0:
        return np.array([0.5, 0.5])  # Uniform if no mass
    # Return normalized + and - probabilities
    return np.array([y[d['+']] / tot, y[d['-']] / tot])


def dump_message_checks(crn, y):
    """
    Print all message concentrations for debugging.

    Useful for comparing CRN messages with standard BP messages.

    Args:
        crn: NappCRN object
        y: species concentration vector
    """
    # Display S-messages (factor → variable)
    print("S_{a->i} (normalized):")
    for (a, i), d in crn.S_ai.items():
        p = read_normalized_triplet(y, d)
        print(f"  S_{{{a}->{i}}}:  [{p[0]:.6f}, {p[1]:.6f}]")

    # Display P-messages (variable → factor)
    print("\nP_{i->a} (normalized):")
    for (i, a), d in crn.P_ia.items():
        p = read_normalized_triplet(y, d)
        print(f"  P_{{{i}->{a}}}:  [{p[0]:.6f}, {p[1]:.6f}]")


def verify_crn_matches_bp(
    fg,
    kappa_r=0.5,
    kappa_prod=200.0,
    t_max=4000.0,
    eps=3e-6,
    stop_when_zero_below=None,
    bp_damping=0.1,
    bp_tol=1e-12,
    verbose=True,
    show_diagnostics=True,
):
    """
    Compare CRN-based BP with standard BP to verify correctness.

    This function runs both implementations on the same factor graph and
    compares the resulting marginal beliefs. If the CRN is working correctly,
    the marginals should match to high precision.

    Args:
        fg: FactorGraph to test
        kappa_r: recycling rate for CRN
        kappa_prod: product computation rate for CRN
        t_max: maximum simulation time for CRN
        eps: initial perturbation for CRN symmetry breaking
        stop_when_zero_below: early stopping threshold (None = no early stop)
        bp_damping: damping parameter for standard BP
        bp_tol: convergence tolerance for standard BP
        verbose: if True, print comparison table
        show_diagnostics: if True, print CRN solver diagnostics

    Returns:
        tuple of (diffs, vb, m_crn):
        - diffs: dict mapping variable → absolute difference in P(+)
        - vb: standard BP variable beliefs
        - m_crn: CRN marginal beliefs
    """
    # ===== RUN STANDARD BP =====
    vb, fb, iters = run_sum_product(fg, max_iter=2000, tol=bp_tol, damping=bp_damping)

    # ===== RUN CRN-BASED BP =====
    crn = NappCRN(fg, kappa_r=kappa_r, kappa_prod=kappa_prod)

    # Simulate CRN dynamics to equilibrium
    sol = crn.simulate(t_max=t_max, rtol=1e-9, atol=1e-12, eps=eps,
                       stop_when_zero_below=stop_when_zero_below)

    # Extract final concentrations
    y_star = sol.y[:, -1]

    # Read marginals from CRN concentrations
    m_crn = crn.read_marginals(y_star)

    # ===== OPTIONAL DIAGNOSTICS =====
    if show_diagnostics:
        # Check convergence quality: report largest remaining 0 pool
        zero_ids = crn._all_zero_pool_indices()
        max_zero = float(np.max(y_star[zero_ids])) if len(zero_ids) else float('nan')
        print(f"[CRN] status={sol.status}, t_end={sol.t[-1]:.3g}, max(0-pool)={max_zero:.3e}")
        # status=0 means success
        # max(0-pool) near zero indicates good convergence

    # ===== COMPARE RESULTS =====
    # Compute absolute differences in P(+) for each variable
    diffs = {}
    for v in fg.vars:
        # Find index of '+' state (robust to different domain orderings)
        plus_idx = fg.dom[v].index('+')

        # Extract P(+) from both methods
        bp_plus = vb[v][plus_idx]
        crn_plus = m_crn[v][fg.dom[v][plus_idx]]

        # Compute absolute error
        diffs[v] = abs(bp_plus - crn_plus)

    # ===== PRINT COMPARISON TABLE =====
    if verbose:
        print("BP vs CRN variable beliefs (P(+) for '+' state):")
        for v in fg.vars:
            plus_idx = fg.dom[v].index('+')
            bp_val = vb[v][plus_idx]
            crn_val = m_crn[v][fg.dom[v][plus_idx]]
            diff = diffs[v]
            print(f"  {v}:  BP={bp_val:.6f}   CRN={crn_val:.6f}   |Δ|={diff:.2e}")

        print(f"Max abs diff: {max(diffs.values()):.3e}")

    return diffs, vb, m_crn

"""
Factor Graph Retraction and Simplification

This module implements graph retraction operations that simplify factor graphs while
preserving their probabilistic structure. Retraction systematically removes "simple"
variables and factors, folding their contributions into neighboring elements.

Key concepts:
- Linear variable: a variable connected to only one pairwise factor (leaf node)
- Colinear factor: a unary factor (can be absorbed into variable potentials)
- Colinear triplet: a chain u--a--x--b--w where x has degree 2 (can be fused)

The retraction process reduces a graph to its "core" - the minimal subgraph containing
all essential structure. Running BP on the core gives the same marginals for core
variables as running BP on the original graph.

Applications:
- Computational efficiency: smaller graphs are faster to solve
- Understanding structure: the core reveals the essential complexity
- Preprocessing: simplify before running expensive inference algorithms
"""

import numpy as np
import collections
from collections import defaultdict


# ========== CONVERSION UTILITIES ==========

def _as_unary_array(fg, v, psi_dict):
    """
    Convert unary factor potential from dict to numpy array.

    The array is ordered according to fg.dom[v], ensuring consistent indexing.

    Args:
        fg: FactorGraph object
        v: variable name
        psi_dict: dictionary mapping domain values to potentials, e.g., {'+': 1.5, '-': 0.8}

    Returns:
        numpy array [ψ(val_0), ψ(val_1), ...] in domain order
    """
    return np.array([psi_dict[val] for val in fg.dom[v]], dtype=float)


def _as_pair_table(fg, a_name):
    """
    Convert pairwise factor potential to numpy 2D array.

    Args:
        fg: FactorGraph object
        a_name: name of pairwise factor

    Returns:
        tuple of ((u, v), T) where:
        - (u, v): ordered variable names in factor scope
        - T: 2D numpy array with T[i,j] = ψ_a(u=val_i, v=val_j)
    """
    f = fg.factor_by_name[a_name]
    assert len(f.scope) == 2, "pairwise only here"

    u, v = f.scope

    # Build index mappings: domain value → array index
    ord_map = {
        u: {val: i for i, val in enumerate(fg.dom[u])},
        v: {val: i for i, val in enumerate(fg.dom[v])}
    }

    # Create empty table with appropriate shape
    T = np.zeros((len(fg.dom[u]), len(fg.dom[v])), dtype=float)

    # Fill table from factor's potential dictionary
    for asg, w in f.psi.items():
        if isinstance(asg, dict):
            uu, vv = asg[u], asg[v]
        else:
            # Assume tuple (u_val, v_val) in scope order
            uu, vv = asg
        T[ord_map[u][uu], ord_map[v][vv]] = float(w)

    return (u, v), T


def _new_unary_factor(name, v, arr, fg):
    """
    Create a unary Factor object from a numpy array.

    Args:
        name: factor name
        v: variable name
        arr: numpy array of potentials in fg.dom[v] order
        fg: FactorGraph object (for domain info)

    Returns:
        Factor object with scope (v,)
    """
    # Convert array back to dict format
    psi = {fg.dom[v][i]: float(arr[i]) for i in range(len(arr))}
    return Factor(name, (v,), psi)


def _new_pair_factor(name, u, v, table, fg):
    """
    Create a pairwise Factor object from a 2D numpy array.

    Args:
        name: factor name
        u, v: variable names (order matters!)
        table: 2D numpy array with table[i,j] = ψ(u=val_i, v=val_j)
        fg: FactorGraph object (for domain info)

    Returns:
        Factor object with scope (u, v)
    """
    psi = {}
    for iu, val_u in enumerate(fg.dom[u]):
        for iv, val_v in enumerate(fg.dom[v]):
            psi[(val_u, val_v)] = float(table[iu, iv])
    return Factor(name, (u, v), psi)



def _collect_unaries(fg):
    """
    Identify which variables have explicit unary factors and collect their potentials.

    Args:
        fg: FactorGraph object

    Returns:
        tuple of (has_unary, phi):
        - has_unary: dict mapping variable → bool (True if has explicit unary factor)
        - phi: dict mapping variable → product of all unary potentials
    """
    has_un = {v: False for v in fg.vars}
    phi = {v: np.ones(len(fg.dom[v]), dtype=float) for v in fg.vars}

    for f in fg.factors:
        if len(f.scope) == 1:
            v = f.scope[0]
            has_un[v] = True
            phi[v] *= _as_unary_array(fg, v, f.psi)

    return has_un, phi


def _variable_degree(fg):
    """
    Compute degree of each variable (number of incident factors of any arity).

    Args:
        fg: FactorGraph object

    Returns:
        dict mapping variable → degree
    """
    return {v: len([a for a in fg.Nv[v] if len(fg.factor_by_name[a].scope) >= 1])
            for v in fg.vars}


def _factor_degree(fg):
    """
    Compute arity of each factor (number of variables in scope).

    Args:
        fg: FactorGraph object

    Returns:
        dict mapping factor_name → arity
    """
    return {a: len(fg.factor_by_name[a].scope) for a in fg.factor_by_name}


# ========== DETECTING RETRACTABLE STRUCTURES ==========

def _variable_degree_pairwise(fg):
    """
    Compute degree counting only pairwise neighbors (excluding unaries).

    This is the relevant degree for identifying linear variables and colinear chains.
    A variable with pairwise degree 1 is a "leaf" that can be retracted.
    A variable with pairwise degree 2 that's interior to a chain can be fused.

    Args:
        fg: FactorGraph object

    Returns:
        dict mapping variable → pairwise degree
    """
    deg = {}
    for v in fg.vars:
        cnt = 0
        for a in fg.Nv[v]:
            # Count only pairwise factors (arity = 2)
            if len(fg.factor_by_name[a].scope) == 2:
                cnt += 1
        deg[v] = cnt
    return deg


def find_linear_variables(fg):
    """
    Find all "linear" variables - those connected to exactly one pairwise factor.

    Linear variables are leaves in the pairwise factor graph and can be efficiently
    retracted by marginalizing them into their single neighbor.

    Args:
        fg: FactorGraph object

    Returns:
        list of variable names with pairwise degree 1
    """
    deg = {}
    for v in fg.vars:
        # Count pairwise neighbors only
        deg[v] = sum(1 for a in fg.Nv[v] if len(fg.factor_by_name[a].scope) == 2)

    return [v for v, d in deg.items() if d == 1]



def _are_variables_connected(fg, start, end, excluding_vars=None):
    """
    Check if two variables are connected by any path, excluding certain variables.

    This uses BFS on the factor graph's variable-factor bipartite structure.
    Useful for detecting whether fusing a chain would create multi-edges.

    Args:
        fg: FactorGraph object
        start, end: variable names to check connectivity between
        excluding_vars: set of variables to ignore (e.g., the middle variable in a chain)

    Returns:
        bool: True if there's a path from start to end not using excluded variables
    """
    if excluding_vars is None:
        excluding_vars = set()

    # Quick check: direct connection via a factor?
    start_neighbors = set()
    for factor_name in fg.Nv[start]:
        factor = fg.factor_by_name[factor_name]
        for var in factor.scope:
            if var != start and var not in excluding_vars:
                start_neighbors.add(var)

    if end in start_neighbors:
        return True

    # Full BFS to find any path
    visited = set([start])
    queue = collections.deque([start])

    while queue:
        current = queue.popleft()

        # Explore neighbors via all incident factors
        for factor_name in fg.Nv[current]:
            factor = fg.factor_by_name[factor_name]
            for neighbor in factor.scope:
                # Skip current variable and excluded variables
                if neighbor == current or neighbor in excluding_vars:
                    continue

                # Found the target!
                if neighbor == end:
                    return True

                # Add to search frontier
                if neighbor not in visited:
                    visited.add(neighbor)
                    queue.append(neighbor)

    return False


# ========== FACTOR GRAPH CLONING AND STATE MANAGEMENT ==========

def _ensure_phi_override(fg):
    """
    Ensure the factor graph has a _phi_override dictionary for storing retraction results.

    The _phi_override dict stores accumulated unary potentials from retracted variables,
    allowing us to fold contributions without creating explicit unary factors.

    Args:
        fg: FactorGraph object (modified in-place)
    """
    if not hasattr(fg, "_phi_override"):
        fg._phi_override = {}


def _clone_fg_with(fg, variables=None, factors=None):
    """
    Create a new FactorGraph with specified variables and factors.

    This preserves the domain definitions and copies relevant _phi_override entries.
    Used after retraction operations to create the simplified graph.

    Args:
        fg: source FactorGraph
        variables: list of variable names (defaults to fg.vars)
        factors: list of Factor objects (defaults to fg.factors)

    Returns:
        new FactorGraph with copied structure and preserved overrides
    """
    variables = fg.vars if variables is None else variables
    factors = fg.factors if factors is None else factors

    # Create new graph (no unaries argument - we handle via overrides)
    new_fg = FactorGraph(variables=variables, domains=fg.dom,
                         factors=factors, unaries=None)

    # Copy only the phi overrides for surviving variables
    if hasattr(fg, "_phi_override"):
        new_fg._phi_override = {v: arr.copy()
                                for v, arr in fg._phi_override.items()
                                if v in variables}

    return new_fg


def get_phi_all(fg):
    """Return φ_v that includes *both* explicit unary factors and _phi_override, in fg.dom[v] order."""
    phi = {v: np.ones(len(fg.dom[v]), dtype=float) for v in fg.vars}
    # explicit unaries
    for f in fg.factors:
        if len(f.scope) == 1:
            v = f.scope[0]
            phi[v] *= np.array([f.psi[val] for val in fg.dom[v]], dtype=float)
    # overrides
    if hasattr(fg, "_phi_override"):
        for v, arr in fg._phi_override.items():
            if v in phi:
                phi[v] *= arr
    return phi

def _phi_multiply(fg, v, arr):
    """Accumulate into off-graph φ override for v."""
    if not hasattr(fg, "_phi_override"):
        fg._phi_override = {}
    arr = np.asarray(arr, dtype=float)
    if v in fg._phi_override:
        fg._phi_override[v] *= arr
    else:
        fg._phi_override[v] = arr.copy()

def fold_all_unaries_into_overrides(fg):
    G = _clone_fg_with(fg)
    for f in list(G.factors):
        if len(f.scope) == 1:
            v = f.scope[0]
            arr = np.array([f.psi[val] for val in G.dom[v]], dtype=float)
            _phi_multiply(G, v, arr)
    keep = [f for f in G.factors if len(f.scope) > 1]
    return _clone_fg_with(G, factors=keep)



# ========== RETRACTION OPERATIONS ==========

def retract_colinear_unary(fg, a_name):
    f = fg.factor_by_name[a_name]
    assert len(f.scope) == 1
    v = f.scope[0]
    arr = np.array([f.psi[val] for val in fg.dom[v]], dtype=float)
    _phi_multiply(fg, v, arr)                 # add this unary exactly once
    keep = [g for g in fg.factors if g.name != a_name]  # drop just this unary
    return _clone_fg_with(fg, factors=keep)   # clone carries overrides (no reapply)



def retract_linear_variable(fg, v, name_prefix="ret"):
    assert v in fg.vars
    pair_neighs = [a for a in fg.Nv[v] if len(fg.factor_by_name[a].scope) == 2]
    assert len(pair_neighs) == 1
    a_name = pair_neighs[0]
    a = fg.factor_by_name[a_name]
    u = a.scope[0] if a.scope[1] == v else a.scope[1]

    # φ including overrides
    phi_all = get_phi_all(fg)
    psi_v = phi_all[v]               # includes v’s explicit + override
    # explicit-only unaries on u
    _, phi_expl = _collect_unaries(fg)
    explicit_u = phi_expl[u]         # explicit unaries on u (no overrides)

    # ψ_a in (v,u) order
    (p, q), T = _as_pair_table(fg, a_name)
    if (p, q) == (u, v):
        T = T.T

    # message term from v → u
    term = (T.T @ psi_v)             # shape = |u|

    # build new factors: drop a, anything with v, and explicit unaries on u
    new_factors = []
    for f in fg.factors:
        if f.name == a_name: continue
        if v in f.scope: continue
        if len(f.scope) == 1 and f.scope[0] == u: continue
        new_factors.append(f)

    new_vars = [x for x in fg.vars if x != v]
    G = _clone_fg_with(fg, variables=new_vars, factors=new_factors)
    # Clone already copied old override_u; now multiply the missing parts:
    _phi_multiply(G, u, explicit_u * term)
    return G


def find_unary_factors(fg):
    return [f.name for f in fg.factors if len(f.scope)==1]

def fold_unary_into_override(fg, a_name):
    f = fg.factor_by_name[a_name]
    assert len(f.scope) == 1
    v = f.scope[0]
    arr = np.array([f.psi[val] for val in fg.dom[v]], dtype=float)
    _phi_multiply(fg, v, arr)                     # apply once
    keep = [g for g in fg.factors if g.name != a_name]
    return _clone_fg_with(fg, factors=keep)       # DO NOT multiply again after clone




# ========== RETRACTABILITY PREDICATES ==========



def is_linear_variable(fg, v):
    """
    Check if a variable is linear (has degree 1 in the bipartite graph).

    Args:
        fg: FactorGraph object
        v: variable name

    Returns:
        bool: True if v has exactly one incident factor
    """
    return len(fg.Nv[v]) == 1


def is_colinear_factor(fg, a):
    """
    Check if a factor is colinear (unary - has arity 1).

    Args:
        fg: FactorGraph object
        a: factor name

    Returns:
        bool: True if factor has exactly one variable in scope
    """
    return len(fg.factor_by_name[a].scope) == 1


def find_colinear_factors(fg):
    """
    Find all unary factors in the graph.

    Args:
        fg: FactorGraph object

    Returns:
        list of factor names with arity 1
    """
    return [a for a, f in fg.factor_by_name.items() if is_colinear_factor(fg, a)]

def find_unary_factors(fg):
    return [f.name for f in fg.factors if len(f.scope) == 1]


# ========== ITERATIVE RETRACTION TO CORE ==========

def iterate_to_core(fg, max_steps=10_000, verbose=False):
    """
    Iteratively retract the factor graph until no more retractions are possible.

    This reduces the graph to its "core" - the minimal subgraph that cannot be
    further simplified. The core preserves all the essential structure and loops.

    The algorithm applies retractions in order of increasing complexity:
    1. Linear variables (degree 1) - simplest, always safe
    2. Colinear chains (degree 2 variables between pairwise factors) - more complex

    Each retraction folds the removed structure into neighboring elements via
    phi overrides, so the core graph encodes the same probability distribution.

    Args:
        fg: FactorGraph object to retract
        max_steps: maximum number of retraction steps (safety limit)
        verbose: if True, print progress information

    Returns:
        tuple of (core, history):
        - core: FactorGraph representing the retracted core
        - history: list of dicts recording each retraction step
    """
    G = _clone_fg_with(fg)  # start from a clone to preserve overrides
    history = []
    for _ in range(max_steps):
        # 1) retract linear variables (leafs wrt pairwise structure)
        linear = find_linear_variables(G)  # the pairwise-degree version
        if linear:
            v = linear[0]
            if verbose: print(f"[retract] linear variable {v}")
            G = retract_linear_variable(G, v)
            history.append({"kind": "linear_var", "v": v})
            continue
        # 2) fold any unary factors remaining (true 'colinear' in the paper)
        unaries = find_unary_factors(G)
        if unaries:
            a = unaries[0]
            if verbose: print(f"[retract] colinear unary {a}")
            G = fold_unary_into_override(G, a)
            history.append({"kind": "colinear_unary", "a": a})
            continue
        break
    return G, history


# ========== VERIFICATION AND COMPARISON ==========

def compare_original_vs_retracted(fg, verbose=True, **bp_kwargs):
    """
    Verify that retraction preserves beliefs by comparing BP on original vs. core.

    This function:
    1. Retracts the graph to its core
    2. Runs BP on the original graph
    3. Runs BP on the core graph
    4. Compares marginal beliefs for core variables

    If retraction is correct, the marginals should match exactly (up to numerical error).

    Args:
        fg: FactorGraph object to test
        verbose: if True, print detailed comparison
        **bp_kwargs: additional arguments for run_sum_product

    Returns:
        tuple of (core, hist, diffs):
        - core: retracted FactorGraph
        - hist: list of retraction steps
        - diffs: dict mapping core variable → absolute difference in P(+)
    """
    # Retract to core
    core, hist = iterate_to_core(fg, verbose=False)

    # Default BP parameters, overridden by kwargs
    bp_params = {"max_iter": 2000, "tol": 1e-12, "damping": 0.1} | bp_kwargs

    # Run BP on both graphs
    vb_full, _, _ = run_sum_product(fg, **bp_params)
    vb_core, _, _ = run_sum_product(core, **bp_params)

    # Compare beliefs on core variables
    diffs = {}
    for v in core.vars:
        dom = core.dom[v]
        # Robustly find '+' state index
        idx_plus = dom.index('+') if '+' in dom else 0

        bp_plus_full = vb_full[v][idx_plus]
        bp_plus_core = vb_core[v][idx_plus]

        diffs[v] = abs(bp_plus_full - bp_plus_core)

    # Print results
    if verbose:
        print(f"[core] vars={core.vars}, factors={[f.name for f in core.factors]}")
        print(f"[retractions] {len(hist)} steps:",
              hist[:5], "..." if len(hist) > 5 else "")
        print("Belief diffs on core vars (P(+)):")
        for v in core.vars:
            dom = core.dom[v]
            idx_plus = dom.index('+') if '+' in dom else 0
            full_val = vb_full[v][idx_plus]
            core_val = vb_core[v][idx_plus]
            diff = diffs[v]
            print(f"  {v}: orig={full_val:.6f}  core={core_val:.6f}  |Δ|={diff:.2e}")

        max_diff = max(diffs.values()) if diffs else 0
        print(f"max |Δ| = {max_diff:.3e}")

    return core, hist, diffs

# Using the builders from your BP snippet:
fgA = build_example_A()
fgB = build_example_B()
fgC = build_example_C()
fgD = build_example_D()
fgE = build_example_E()
fgF = build_example_F()

print("=== Example A ===")
verify_crn_matches_bp(fgA)

print("\n=== Example B ===")
verify_crn_matches_bp(fgB)

print("\n=== Example C ===")
verify_crn_matches_bp(fgC)

print("\n=== Example D ===")
verify_crn_matches_bp(fgD)

print("\n=== Example E ===")
verify_crn_matches_bp(fgE)

print("\n=== Example F ===")
verify_crn_matches_bp(fgF)

def sweep_crn_params(
    fg,
    kappa_r_grid=(0.1, 0.5, 1.0),
    kappa_prod_grid=(10, 50, 100),
    *,
    t_max=4000.0,
    stop_when_zero_below=1e-4,
    bp_max_iter=2000,
    bp_tol=1e-12,
    bp_damping=0.1,
    crn_extra_kwargs=None,     # e.g. dict(rtol=1e-9, atol=1e-12, eps=3e-6)
    quiet=False
):
    """
    Sweep (kappa_r, kappa_prod) for any factor graph fg.
    Returns a list of dicts with diffs per variable and max diff per combo.
    """
    crn_extra_kwargs = crn_extra_kwargs or {}

    # --- 1) Run BP once on this graph ---
    vb, fb, _ = run_sum_product(fg, max_iter=bp_max_iter, tol=bp_tol, damping=bp_damping)

    # robustly pick the '+' index if it exists; else just use index 0
    plus_index = {}
    plus_symbol = {}
    for v in fg.vars:
        dom = fg.dom[v]
        if '+' in dom:
            plus_index[v] = dom.index('+')
            plus_symbol[v] = '+'
        else:
            plus_index[v] = 0
            plus_symbol[v] = dom[0]  # best-effort

    results = []
    for kappa_r in kappa_r_grid:
        for kappa_prod in kappa_prod_grid:
            # --- 2) Build & simulate CRN for this combo ---
            crn = NappCRN(fg, kappa_r=kappa_r, kappa_prod=kappa_prod)
            sol = crn.simulate(t_max=t_max, stop_when_zero_below=stop_when_zero_below, **crn_extra_kwargs)
            y_star = sol.y[:, -1]
            m_crn = crn.read_marginals(y_star)

            # --- 3) Compare marginals (P(+) or first state if '+' absent) ---
            diffs = {}
            for v in fg.vars:
                bp_plus  = vb[v][plus_index[v]]
                crn_plus = m_crn[v][fg.dom[v][plus_index[v]]]
                diffs[v] = abs(bp_plus - crn_plus)

            entry = {
                "kappa_r": kappa_r,
                "kappa_prod": kappa_prod,
                "max_abs_diff": max(diffs.values()) if diffs else float("nan"),
                "diffs": diffs,
                "bp_beliefs": {v: vb[v].copy() for v in fg.vars},
                "crn_marginals": m_crn,
                "solver_status": sol.status,      # 0 success, >0 event/termination
                "t_end": sol.t[-1],
            }
            results.append(entry)

            if not quiet:
                print(f"\nkappa_r={kappa_r}, kappa_prod={kappa_prod}")
                print(f"[CRN] status={sol.status}, t_end={sol.t[-1]:.3g}")
                print("BP vs CRN variable beliefs (P(+) for '+'/first-state):")
                for v in fg.vars:
                    bp_plus  = vb[v][plus_index[v]]
                    crn_plus = m_crn[v][fg.dom[v][plus_index[v]]]
                    print(f"  {v}:  BP={bp_plus:.6f}   CRN={crn_plus:.6f}   |Δ|={diffs[v]:.2e}")
                print(f"Max abs diff: {entry['max_abs_diff']:.3e}")

    return results

# pick any of your graphs
fg = fgD  # or fgA, fgC, fgD, fgE, fgF, ...

res = sweep_crn_params(
    fg,
    kappa_r_grid=[0.1, 0.5, 1.0],
    kappa_prod_grid=[10, 50, 100],
    t_max=4000.0,
    stop_when_zero_below=1e-4,
    crn_extra_kwargs=dict(rtol=1e-9, atol=1e-12, eps=3e-6),
)

# Access best setting by smallest max_abs_diff:
best = min(res, key=lambda r: r["max_abs_diff"])
print("\nBest combo:", best["kappa_r"], best["kappa_prod"], "max diff:", best["max_abs_diff"])

def _dump_phi(fg, tag):
    phi = get_phi_all(fg)
    print(tag, {v: tuple(np.round(phi[v], 6)) for v in fg.vars})
# Example:
_dump_phi(fgA, "orig φ")
# If you just want the core graph:
core_fg, steps = iterate_to_core(fgA, verbose=True)
_dump_phi(core_fg, "core φ")
print("Core vars:", core_fg.vars)
print("Core factors:", [f.name for f in core_fg.factors])

# You can still feed 'core_fg' to your CRN builder to compare CRN@core vs BP@core.

# ---------- CRN vs. CRN(core) comparison ----------

def _apply_phi_overrides_to_crn_unaries(crn):
    """
    Make NappCRN aware of off-graph φ overrides stored on the FactorGraph.
    We multiply psi_ii[i] by fg._phi_override[i] if present.
    """
    fg = crn.fg
    if hasattr(fg, "_phi_override"):
        for i, arr in fg._phi_override.items():
            if i in crn.psi_ii:
                # arr is in fg.dom[i] order; psi_ii[i] is length-2 array
                crn.psi_ii[i] = crn.psi_ii[i] * np.asarray(arr, dtype=float)

def _simulate_crn(crn, t_max=4000.0, kappa_r=0.5, kappa_prod=200.0,
                  rtol=1e-9, atol=1e-12, eps=3e-6, stop_when_zero_below=1e-9):
    sol = crn.simulate(t_max=t_max, rtol=rtol, atol=atol,
                       eps=eps, stop_when_zero_below=stop_when_zero_below)
    y_star = sol.y[:, -1]
    m = crn.read_marginals(y_star)
    # also return some diagnostics
    zero_ids = crn._all_zero_pool_indices()
    max_zero = float(np.max(y_star[zero_ids])) if len(zero_ids) else float('nan')
    return m, dict(status=sol.status, t_end=float(sol.t[-1]), max_zero_pool=max_zero)

def compare_crn_vs_core(
    fg,
    kappa_r=0.5,
    kappa_prod=200.0,
    t_max=4000.0,
    rtol=1e-9,
    atol=1e-12,
    eps=3e-6,
    stop_when_zero_below=1e-9,
    verbose=True
):
    """
    Build CRNs for the original factor graph and for its reduced core,
    simulate both to steady state, and compare variable marginals on the core vars.

    Returns:
        core_fg, history, diffs, marginals_full, marginals_core, diag_full, diag_core
    """
    # 1) retract to core
    core_fg, history = iterate_to_core(fg, verbose=False)

    # 2) build CRNs
    crn_full = NappCRN(fg, kappa_r=kappa_r, kappa_prod=kappa_prod)
    _apply_phi_overrides_to_crn_unaries(crn_full)  # fold off-graph φ into S_ii

    crn_core = NappCRN(core_fg, kappa_r=kappa_r, kappa_prod=kappa_prod)
    _apply_phi_overrides_to_crn_unaries(crn_core)

    # 3) simulate both
    m_full, diag_full = _simulate_crn(crn_full, t_max, kappa_r, kappa_prod, rtol, atol, eps, stop_when_zero_below)
    m_core, diag_core = _simulate_crn(crn_core, t_max, kappa_r, kappa_prod, rtol, atol, eps, stop_when_zero_below)

    # 4) compare on core variables (use '+' entry in each var's domain)
    diffs = {}
    for v in core_fg.vars:
        dom = core_fg.dom[v]
        plus_sym = dom[0] if dom[0] == '+' or '+' in dom else dom[0]  # robust
        if '+' in dom:
            plus_sym = '+'
        p_full = m_full[v][plus_sym]
        p_core = m_core[v][plus_sym]
        diffs[v] = abs(p_full - p_core)

    if verbose:
        print("[core] vars:", core_fg.vars, "factors:", [f.name for f in core_fg.factors])
        print(f"[full CRN]  status={diag_full['status']}  t_end={diag_full['t_end']:.3g}  max(0-pool)={diag_full['max_zero_pool']:.3e}")
        print(f"[core CRN]  status={diag_core['status']}  t_end={diag_core['t_end']:.3g}  max(0-pool)={diag_core['max_zero_pool']:.3e}")
        print("\nVariable marginals P(+) on core vars:")
        for v in core_fg.vars:
            dom = core_fg.dom[v]
            plus = '+' if '+' in dom else dom[0]
            print(f"  {v}:  full={m_full[v][plus]:.6f}   core={m_core[v][plus]:.6f}   |Δ|={diffs[v]:.2e}")
        if diffs:
            print(f"Max |Δ| across core vars: {max(diffs.values()):.3e}")

    return core_fg, history, diffs, m_full, m_core, diag_full, diag_core

# pick any fgX you already have
core, hist, diffs = compare_original_vs_retracted(fgC, verbose=True)

if __name__ == "__main__":
    # pick any builder you already have
    fg = build_example_C()  # for instance

    core_fg, hist, diffs, m_full, m_core, diag_full, diag_core = compare_crn_vs_core(
        fg,
        kappa_r=0.5,
        kappa_prod=200.0,
        t_max=4000.0,
        stop_when_zero_below=1e-9,
        verbose=True
    )

# ---------- Stress test: BP/CRN vs core on many graphs ----------

def build_random_example(name="Rand", n=6, edge_p=0.35, field_p=0.5, J_scale=0.8, h_scale=0.6, seed=0):
    rng = np.random.default_rng(seed)
    vars_ = [f"s{i+1}" for i in range(n)]
    dom = {v: ('+','-') for v in vars_}
    factors = []
    # random edges
    for i in range(n):
        for j in range(i+1, n):
            if rng.random() < edge_p:
                J = (2*rng.random()-1)*J_scale  # uniform in [-J_scale, J_scale]
                factors.append(Factor(f"f{ i+1 }_{ j+1 }", (vars_[i], vars_[j]), ising_pair(J=J)))
    # ensure connected-ish: add at least a chain backbone
    for i in range(n-1):
        if not any(set((vars_[i], vars_[i+1]))==set(f.scope) for f in factors):
            J = (2*rng.random()-1)*J_scale
            factors.append(Factor(f"bk_{i+1}_{i+2}", (vars_[i], vars_[i+1]), ising_pair(J=J)))
    # random unaries
    un = {}
    for v in vars_:
        if rng.random() < field_p:
            h = (2*rng.random()-1)*h_scale
            un[v] = ising_unary(h=h)
    return FactorGraph(variables=vars_, domains=dom, factors=factors, unaries=un if un else None), name

def _fmt_core(core_fg):
    return f"vars={core_fg.vars}, factors={[f.name for f in core_fg.factors]}"

def run_case(name, fg, *, bp_tol=1e-12, crn_tol=1e-10, kappa_r=0.5, kappa_prod=200.0, t_max=4000.0, verbose=True):
    print(f"\n=== {name} ===")
    # BP: original vs retracted
    core_fg, hist, bp_diffs = compare_original_vs_retracted(fg, verbose=False, tol=bp_tol, damping=0.1, max_iter=2000)
    max_bp = max(bp_diffs.values()) if bp_diffs else 0.0
    print(f"[core (BP)] {_fmt_core(core_fg)}    max |Δ_BP|={max_bp:.3e}   steps={len(hist)}")
    # CRN: FG vs core(FG)
    core2, hist2, crn_diffs, m_full, m_core, diag_full, diag_core = compare_crn_vs_core(
        fg,
        kappa_r=kappa_r, kappa_prod=kappa_prod, t_max=t_max,
        stop_when_zero_below=1e-9, verbose=False
    )
    assert core2.vars == core_fg.vars and len(core2.factors) == len(core_fg.factors), "Core mismatch between BP and CRN runs."
    max_crn = max(crn_diffs.values()) if crn_diffs else 0.0
    print(f"[core (CRN)] {_fmt_core(core2)}   max |Δ_CRN|={max_crn:.3e}   CRN0={diag_full['max_zero_pool']:.2e}  CRN0core={diag_core['max_zero_pool']:.2e}")
    # hard checks
    assert max_bp <= bp_tol*10, f"BP core mismatch too large: {max_bp}"
    assert max_crn <= crn_tol,  f"CRN vs CRN(core) mismatch too large: {max_crn}"
    if verbose:
        for v in core_fg.vars:
            plus = '+' if '+' in core_fg.dom[v] else core_fg.dom[v][0]
            print(f"  {v}: BP(core)~BP(orig)  Δ={bp_diffs[v]:.2e}   CRN(full)={m_full[v][plus]:.6f}  CRN(core)={m_core[v][plus]:.6f}  Δ={crn_diffs[v]:.2e}")

def stress_test_suite():
    cases = [
        ("Example A (triangle + leaf + unary)", build_example_A()),
        ("Example B (square + leaf + unary)",   build_example_B()),
        ("Example C (chain + unary)",           build_example_C()),
        ("Example D (4-cycle + strong unaries)",build_example_D()),
        ("Example E (chain + strong unaries)",  build_example_E()),
        ("Example F (chain, no unaries)",       build_example_F()),
    ]
    # randomized families
    rand1, _ = build_random_example("Rand-6", n=6, edge_p=0.35, field_p=0.5, seed=31)
    rand2, _ = build_random_example("Rand-8-sparser", n=8, edge_p=0.25, field_p=0.4, seed=2)
    rand3, _ = build_random_example("Rand-10-denser", n=10, edge_p=0.5, field_p=0.6, seed=33)
    cases += [("Random-6", rand1), ("Random-8", rand2), ("Random-10", rand3)]

    failures = []
    for name, fg in cases:
        try:
            run_case(name, fg, bp_tol=1e-12, crn_tol=1e-10, kappa_r=0.5, kappa_prod=300.0, t_max=6000.0, verbose=True)
        except AssertionError as e:
            print(f"  [FAIL] {name}: {e}")
            failures.append((name, str(e)))
    if failures:
        print("\nSummary: Some cases failed tolerance:")
        for n, msg in failures:
            print(f" - {n}: {msg}")
    else:
        print("\nAll cases passed within tolerance.")

# ---- run it
if __name__ == "__main__":
    stress_test_suite()

